# -*- coding: utf-8 -*-
"""wav2vec_layer-10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15-7ocuP5tF9lz8WD-EkVrL30XlqBMjms

**Import required libraries**
"""

# Import necessary libraries

import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.model_selection import cross_val_score
import numpy as np

"""**Mount the Google Drive**"""

# Mount Google Drive

from google.colab import drive
drive.mount('/content/drive')

"""**Load data from csv files**"""

# Define file paths for the datasets
trainData = "/content/drive/MyDrive/wav2vec-layer-10/train.csv"
validData = "/content/drive/MyDrive/wav2vec-layer-10/valid.csv"
testData = "/content/drive/MyDrive/wav2vec-layer-10/test.csv"

# Load data into DataFrames

trainDataFrame = pd.read_csv(trainData)
validDataFrame= pd.read_csv(validData)
testDataFrame = pd.read_csv(testData)

# drop ID column from the test dataset
X_test = testDataFrame.drop('ID', axis=1)

X_test.head()

"""**Splitting the feature data based on different labels and Standardize features**"""

labels = ['label_1', 'label_2', 'label_3', 'label_4']

# Initialize dictionaries to store data

X_train = {}
X_valid = {}
y_train = {}
y_valid = {}

# Loop through labels and perform preprocessing
for label in labels:
    scaler = StandardScaler()
    train_df = trainDataFrame
    validate_df = validDataFrame
    test_df = testDataFrame
    if label == 'label_2': # Remove NaN rows for label_2
      train_df = trainDataFrame[trainDataFrame[label].notna()]
      validate_df = validDataFrame[validDataFrame[label].notna()]


    # Standardize features
    X_train[label] = pd.DataFrame(scaler.fit_transform(train_df.iloc[:, :-4]))
    X_valid[label] = pd.DataFrame(scaler.transform(validate_df.iloc[:, :-4]))

    y_train[label] = train_df[label]
    y_valid[label] = validate_df[label]

X_train['label_1']

"""**Support Vector Classifire**"""

def support_vector_model(label_name, training_data, validation_data, training_labels, validation_labels):
    model = SVC(kernel='linear')
    model.fit(training_data[label_name], training_labels[label_name])
    labelPredictions = model.predict(validation_data[label_name])
    print(f"+ Accuracy Score for {label_name} = {accuracy_score(validation_labels[label_name], labelPredictions)}")

"""**Weighted Support Vector Classifire**"""

def weighted_support_vector_model(label_name, training_data, validation_data, training_labels, validation_labels):
  model = SVC(kernel='linear', class_weight='balanced')
  model.fit(training_data[label_name], training_labels[label_name])
  labelPredictions = model.predict(validation_data[label_name])
  print(f"+ Accuracy Score for {label_name} = {accuracy_score(validation_labels[label_name], labelPredictions)}")

"""**Random Search for Hyperparameter Tuning**"""

from sklearn.model_selection import RandomizedSearchCV

def perform_random_search(model, param_distribution, X_train, y_train):

    # Create a RandomizedSearchCV object
    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distribution, scoring='accuracy', n_iter=5, cv=2, n_jobs=-1, verbose=2)

    # Perform the random search
    random_search.fit(X_train, y_train)

    # Get the best model and best parameters
    best_model = random_search.best_estimator_
    best_params = random_search.best_params_

    return best_model, best_params

"""**Cross Validation**"""

def cross_validation(model, k_value, X, y):

    scores = cross_val_score(model, X, y, cv=k_value)
    mean_score = np.mean(scores)
    std_deviation = np.std(scores)
    print("Cross-Validation Scores:", scores)
    print("Mean Score:", mean_score)
    print("Standard Deviation:", std_deviation)

"""# Label 01"""

plt.figure(figsize=(18,6))
sn.countplot(data=y_train, x = 'label_1', color = 'skyblue')

"""**Testing the performance of the initial dataset after data standardization using evaluation metrics**"""

support_vector_model('label_1', X_train, X_valid, y_train, y_valid)

"""**K best method for feature selection**"""

# Number of new features
new_features = 560
selector = SelectKBest(f_classif, k=new_features)

# Initialize dictionaries for selected features
X_train_selected = {}
X_valid_selected = {}
X_test_selected_l1 = X_test.copy()

X_train_selected['label_1'] = pd.DataFrame(selector.fit_transform(X_train['label_1'], y_train['label_1']))
X_valid_selected['label_1'] = pd.DataFrame(selector.transform(X_valid['label_1']))
X_test_selected_l1 = pd.DataFrame(selector.transform(X_test_selected_l1))

"""**Evaluating accuracy after the process of selecting relevant features using K-Best**"""

support_vector_model('label_1', X_train_selected, X_valid_selected, y_train, y_valid)

"""**Principal Component Analysis for feature selection**"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.99, svd_solver='full')

# Initialize dictionaries for selected features
X_train_pca_selected = {}
X_valid_pca_selected = {}
X_test_pca_selected_l1 = X_test.copy()

X_train_pca_selected['label_1'] = pd.DataFrame(pca.fit_transform(X_train['label_1']))
X_valid_pca_selected['label_1'] = pd.DataFrame(pca.transform(X_valid['label_1']))
X_test_pca_selected_l1 = pd.DataFrame(pca.transform(X_test_pca_selected_l1))

"""**Evaluating accuracy after the process of selecting relevant features using PCA**"""

support_vector_model('label_1', X_train_pca_selected, X_valid_pca_selected, y_train, y_valid)

"""**Cross Validation**"""

model = SVC(kernel= 'linear', gamma= 0.001, C= 1.0)
cross_validation(model, 5, X_train['label_1'], y_train['label_1'])

"""**Hyperparameter Tuning**"""

param_distribution = {
    'C': [100,10,1,0.1,0.01,0.001],
    'kernel': ['linear', 'rbf'],
    'gamma': [0.001, 0.01, 0.1, 1, 10]
}

best_model_l1, best_params_l1 = perform_random_search(SVC(), param_distribution, X_train_selected['label_1'], y_train['label_1'])

best_model_l1

best_params_l1

# best_model.fit(X_train_selected_l1['label_1'], y_train['label_1'])
y_predicition_label_1 = best_model_l1.predict(X_valid_selected['label_1'])
print(f"+ Accuracy Score for label_1 using best model with hyper parameter tuning = {accuracy_score(y_valid['label_1'], y_predicition_label_1)}")

"""# Label 02"""

plt.figure(figsize=(18,6))
sn.countplot(data=y_train, x = 'label_2', color = 'skyblue')

"""**Testing the performance of the initial dataset after data standardization using evaluation metrics**"""

weighted_support_vector_model('label_2', X_train, X_valid, y_train, y_valid)

"""**K best method for feature selection**"""

# Number of new features
new_features = 560
selector = SelectKBest(f_classif, k=new_features)

# Initialize dictionaries for selected features
X_train_selected = {}
X_valid_selected = {}
X_test_selected_l2 = X_test.copy()

X_train_selected['label_2'] = pd.DataFrame(selector.fit_transform(X_train['label_2'], y_train['label_2']))
X_valid_selected['label_2'] = pd.DataFrame(selector.transform(X_valid['label_2']))
X_test_selected_l2 = pd.DataFrame(selector.transform(X_test_selected_l2))

"""**Evaluating accuracy after the process of selecting relevant features using K-Best**"""

weighted_support_vector_model('label_2', X_train_selected, X_valid_selected, y_train, y_valid)

"""**Principal Component Analysis for feature selection**"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.99, svd_solver='full')

# Initialize dictionaries for selected features
X_train_pca_selected = {}
X_valid_pca_selected = {}
X_test_pca_selected_l2 = X_test.copy()

X_train_pca_selected['label_2'] = pd.DataFrame(pca.fit_transform(X_train['label_2']))
X_valid_pca_selected['label_2'] = pd.DataFrame(pca.transform(X_valid['label_2']))
X_test_pca_selected_l2 = pd.DataFrame(pca.transform(X_test_pca_selected_l2))

"""**Evaluating accuracy after the process of selecting relevant features using PCA**"""

weighted_support_vector_model('label_2', X_train_pca_selected, X_valid_pca_selected, y_train, y_valid)

"""**Cross Validation**"""

model = SVC(kernel= 'linear', gamma= 0.001, C= 1.0)
cross_validation(model, 5, X_train['label_2'], y_train['label_2'])

"""**Hyperparameter Tuning**"""

param_distribution = {
    'C': [100,10,1,0.1,0.01,0.001],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': [0.001, 0.01, 0.1, 1, 10],
    'class_weight': ['balanced', None]
}

best_model_l2, best_params_l2 = perform_random_search(SVC(), param_distribution, X_train_pca_selected['label_2'], y_train['label_2'])

best_model_l2

best_params_l2

# best_model.fit(X_train_selected_l1['label_1'], y_train['label_1'])
y_predicition_label_2 = best_model_l2.predict(X_valid_pca_selected['label_2'])
print(f"+ Accuracy Score for label_1 using best model with hyper parameter tuning = {accuracy_score(y_valid['label_2'], y_predicition_label_2)}")

"""# Label 03"""

plt.figure(figsize=(18,6))
sn.countplot(data=y_train, x = 'label_3', color = 'skyblue')

"""**Testing the performance of the initial dataset after data standardization using evaluation metrics**"""

weighted_support_vector_model('label_3', X_train, X_valid, y_train, y_valid)

"""**K best method for feature selection**"""

# Number of new features
new_features = 560
selector = SelectKBest(f_classif, k=new_features)

# Initialize dictionaries for selected features
X_train_selected = {}
X_valid_selected = {}
X_test_selected_l3 = X_test.copy()

X_train_selected['label_3'] = pd.DataFrame(selector.fit_transform(X_train['label_3'], y_train['label_3']))
X_valid_selected['label_3'] = pd.DataFrame(selector.transform(X_valid['label_3']))
X_test_selected_l3 = pd.DataFrame(selector.transform(X_test_selected_l3))

"""**Evaluating accuracy after the process of selecting relevant features using K-Best**"""

weighted_support_vector_model('label_3', X_train_selected, X_valid_selected, y_train, y_valid)

"""**Principal Component Analysis for feature selection**"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.99, svd_solver='full')

# Initialize dictionaries for selected features
X_train_pca_selected = {}
X_valid_pca_selected = {}
X_test_pca_selected_l3 = X_test.copy()

X_train_pca_selected['label_3'] = pd.DataFrame(pca.fit_transform(X_train['label_3']))
X_valid_pca_selected['label_3'] = pd.DataFrame(pca.transform(X_valid['label_3']))
X_test_pca_selected_l3 = pd.DataFrame(pca.transform(X_test_pca_selected_l3))

"""**Evaluating accuracy after the process of selecting relevant features using PCA**"""

weighted_support_vector_model('label_3', X_train_pca_selected, X_valid_pca_selected, y_train, y_valid)

"""**Cross Validation**"""

model = SVC(kernel= 'linear', gamma= 0.001, C= 1.0)
cross_validation(model, 5, X_train_selected['label_3'], y_train['label_3'])

"""**Hyperparameter Tuning**"""

param_distribution = {
    'C': [100,10,1,0.1,0.01,0.001],
    'kernel': ['linear', 'rbf'],
    'gamma': [0.001, 0.01, 0.1, 1, 10],
    'class_weight': ['balanced'],
}

best_model_l3, best_params_l3 = perform_random_search(SVC(), param_distribution, X_train_selected['label_3'], y_train['label_3'])

best_model_l3

best_params_l3

y_predicition_label_3 = best_model_l3.predict(X_valid_selected['label_3'])
print(f"+ Accuracy Score for label_1 using best model with hyper parameter tuning = {accuracy_score(y_valid['label_3'], y_predicition_label_3)}")

"""# Label 04"""

plt.figure(figsize=(18,6))
sn.countplot(data=y_train, x = 'label_4', color = 'skyblue')

"""**Testing the performance of the initial dataset after data standardization using evaluation metrics**"""

weighted_support_vector_model('label_4', X_train, X_valid, y_train, y_valid)

"""**K best method for feature selection**"""

# Number of new features
new_features = 560
selector = SelectKBest(f_classif, k=new_features)

# Initialize dictionaries for selected features
X_train_selected = {}
X_valid_selected = {}
X_test_selected_l4 = X_test.copy()

X_train_selected['label_4'] = pd.DataFrame(selector.fit_transform(X_train['label_4'], y_train['label_4']))
X_valid_selected['label_4'] = pd.DataFrame(selector.transform(X_valid['label_4']))
X_test_selected_l4 = pd.DataFrame(selector.transform(X_test_selected_l4))

"""**Evaluating accuracy after the process of selecting relevant features using K-Best**"""

weighted_support_vector_model('label_4', X_train_selected, X_valid_selected, y_train, y_valid)

"""**Principal Component Analysis for feature selection**"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.97, svd_solver='full')

# Initialize dictionaries for selected features
X_train_pca_selected = {}
X_valid_pca_selected = {}
X_test_pca_selected_l4 = X_test.copy()

X_train_pca_selected['label_4'] = pd.DataFrame(pca.fit_transform(X_train['label_4']))
X_valid_pca_selected['label_4'] = pd.DataFrame(pca.transform(X_valid['label_4']))
X_test_pca_selected_l4 = pd.DataFrame(pca.transform(X_test_pca_selected_l4))

"""**Evaluating accuracy after the process of selecting relevant features using PCA**"""

weighted_support_vector_model('label_4', X_train_pca_selected, X_valid_pca_selected, y_train, y_valid)

"""**Hyperparameter Tuning**"""

param_distribution = {
    'C': [100,10,1,0.1,0.01,0.001],
    'kernel': ['linear', 'rbf'],
    'gamma': [0.001, 0.01, 0.1, 1, 10],
    'class_weight': ['balanced'],
}

best_model_l4, best_params_l4 = perform_random_search(SVC(), param_distribution, X_train_selected['label_4'], y_train['label_4'])

best_model_l4

best_params_l4

y_predicition_label_4 = best_model_l4.predict(X_valid_selected['label_4'])
print(f"+ Accuracy Score for label_4 using best model with hyper parameter tuning = {accuracy_score(y_valid['label_4'], y_predicition_label_4)}")

"""**Cross Validation**"""

model = SVC(kernel= 'linear', gamma= 0.001, C= 1.0)
cross_validation(model, 5, X_train_selected['label_4'], y_train['label_4'])

"""# Generating CSV file"""

# Make predictions for each label using the best models
l1_prediction = best_model_l1.predict(X_test_selected_l4)  # Predictions for label_1
l2_prediction = best_model_l2.predict(X_test_selected_l4)  # Predictions for label_2
l3_prediction = best_model_l3.predict(X_test_selected_l4)  # Predictions for label_3
l4_prediction = best_model_l4.predict(X_test_selected_l4)  # Predictions for label_4

# Extract the 'ID' column from the test data for result reporting
Ids = testDataFrame['ID']

# Create a dictionary to store the results
results = {
    'ID': Ids,          # Store the 'ID' column
    'label_1': l1_prediction,  # Predicted values for label_1
    'label_2': l2_prediction,  # Predicted values for label_2
    'label_3': l3_prediction,  # Predicted values for label_3
    'label_4': l4_prediction   # Predicted values for label_4
}

# Create a DataFrame to organize the results
resultDataFrame = pd.DataFrame(results)

# Save the results to a CSV file ('layer_9.csv') without the index column
resultDataFrame.to_csv('layer_10.csv', index=False)